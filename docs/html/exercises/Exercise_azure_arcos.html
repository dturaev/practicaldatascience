
<!DOCTYPE html>

<html>
  <head>
    <!-- Google Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-133829453-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Importing Opioid Shipment Data on Azure &#8212; Practical Data Science</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="Importing-Opioid-Shipment-Data-on-Azure">
<h1>Importing Opioid Shipment Data on Azure<a class="headerlink" href="#Importing-Opioid-Shipment-Data-on-Azure" title="Permalink to this headline">¶</a></h1>
<p>Yup, it’s time to work with everyone’s favorite data: The DEA ARCOS opioid drug shipment database! But today we’re gonna work with this data not on our personal computers, but instead on a dask cluster running on Microsoft Azure.</p>
<p>For these exercises, you’ll need an Azure account with available credits. The easiest way to do this is to set up a new account (either by signing up as a student, or just using a new gmail account to sign up as a new regular user). <a class="reference external" href="../cloud_azureml.ipynb">You can find instructions for doing so here</a>.</p>
<p>As with our last exercise using dask, to strike a balance between keeping the data big enough to be interesting and small enough you don’t have to wait around too long, we’ll be working with a thinned version that has only January and February from 2011 and 2012. This way the workflow you develop will be same the same workflow you’d use for the full dataset, of course, but we can move quickly, even on the limited resources you’re allowed to use on a free account.</p>
<p><strong>Before you start, make sure to review the package setup instructions for</strong><a class="reference internal" href="../cloud_dask.html"><span class="doc">dask here!</span></a></p>
<div class="section" id="Loading-Data-to-Azure">
<h2>Loading Data to Azure<a class="headerlink" href="#Loading-Data-to-Azure" title="Permalink to this headline">¶</a></h2>
<p><strong>(1)</strong> Download the thinned ARCOS data <a class="reference external" href="https://www.dropbox.com/s/mm0wwwho61pc6q7/arcos_jan_feb_2011_2012.tsv.zip?dl=0">from this link</a>. It should be about 250MB zipped, 2.31 GB unzipped.</p>
<p><strong>(2)</strong> Let’s upload this data to Azure by creating a new Storage Account and Blob Container (you may have already done this while reading about Azure in today’s readings, but let’s do it again for practice!). If you can’t remember how to do it, <a class="reference external" href="https://www.practicaldatascience.org/html/cloud_azureml.html#Storage-on-Azure">you can review instructions here</a>.</p>
<p>If you have an existing Resource Group, you can use that, but if not remember you’ll have to create one of those too!</p>
<p><strong>(3)</strong> Once you have a Blob Container, unzip your data and then start the upload process through the web browser interface (will talk about other tools for managing data tomorrow).</p>
<p>Note you <strong>must</strong> unzip your files first! Annoying, I know. There are ways to upload zipped files and unzip them later, but they’re surprisingly complicated, and dask won’t read a zipped file.</p>
</div>
<div class="section" id="Setting-Up-Dask-on-AzureML">
<h2>Setting Up Dask on AzureML<a class="headerlink" href="#Setting-Up-Dask-on-AzureML" title="Permalink to this headline">¶</a></h2>
<p><strong>(4)</strong> While that’s happening, let’s get ready to start a <code class="docutils literal notranslate"><span class="pre">dask</span></code> cluster. First, let’s create a new Workspace! Again, you probably did this already, but let’s do it again anyway. :) Practice makes perfect. Again, <a class="reference external" href="https://www.practicaldatascience.org/html/cloud_azureml.html#Create-a-Workspace">directions are here if you’ve forgotten</a>.</p>
<p><strong>(5)</strong> Now, following the example <a class="reference internal" href="../cloud_dask.html"><span class="doc">from the readings</span></a>, create a new notebook and write your code to start a dask cluster and get it running!</p>
<p>Since you’re probably using a free student account, use <code class="docutils literal notranslate"><span class="pre">vm_size=&quot;Standard_DS11_v2&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">initial_node_count=3</span></code>. This will keep us to 6 total CPUs (this is not a lot, but it’s what we can do for free!). If you’re using a normal free account, your quota is 4 CPUs, so set your <code class="docutils literal notranslate"><span class="pre">initial_node_count=2</span></code>.</p>
<p>Note we’re using DS11 because <a class="reference external" href="https://docs.microsoft.com/en-us/azure/virtual-machines/dv2-dsv2-series">that model has 2 CPUs per node</a>, so we can stay under our 6 CPU quota for free accounts, and fast storage (the “S” in “DS11” is for Storage-optimized, and for data science, we almost always want fast storage access).</p>
<div class="section" id="A-Note-on-Quotas">
<h3>A Note on Quotas<a class="headerlink" href="#A-Note-on-Quotas" title="Permalink to this headline">¶</a></h3>
<p><strong>Remember that for a free account, Azure imposes a quota of only 6 CPUs for students and 4 CPUs for normal free accounts to use across all your activities on Azure.</strong></p>
<p>Since we’re requesting a cluster with 4 or 6 CPUs (depending on what kind of account you have), if have a VM running <em>anywhere</em> on Azure (even a Stopped VM counts against quota – you have to delete the VM), your AzureMLCluster command will just sit there processing forever until eventually you get a timeout error (with no reference to Quotas).</p>
<p>To check the status of your cluster startup, login to Azure in your browser and navigate to your Machine Learning Workspace. Remember to navigate to the new version by clicking “Launch Studio” if you end up in the old interface (the new interface will have a URL that starts <code class="docutils literal notranslate"><span class="pre">ml.azure.com</span></code>). The click on <code class="docutils literal notranslate"><span class="pre">Compute</span></code> on the left menu towards the bottom and click on the <code class="docutils literal notranslate"><span class="pre">Compute</span> <span class="pre">Clusters</span></code> tab. There you should see the cluster your Python commands are trying to create. There you can see your
quota status by clicking on the <code class="docutils literal notranslate"><span class="pre">View</span> <span class="pre">Quota</span></code> button, though this only appears if you have at least one cluster trying to start up (which is a little annoying):</p>
<p><img alt="azure_quota_button" src="../_images/azure_quota_button.png" /></p>
<p>Also, if you’re hitting your quote limit, you may also see a red circle next to the cluster that’s starting up. If you click on it, you’ll see:</p>
<p><img alt="azure_quota_warning" src="../_images/azure_quota_warning.png" /></p>
<p>If you’re having continued issues, make sure you don’t have any VMs up in your current workspace, and that you haven’t left any VMs open in other workspaces you may have created!</p>
</div>
</div>
<div class="section" id="Actually-Analyzing-Data">
<h2>Actually Analyzing Data<a class="headerlink" href="#Actually-Analyzing-Data" title="Permalink to this headline">¶</a></h2>
<p>OK, is your dask cluster up and running? And has your data upload finished? Then it’s time to start analyzing some data!</p>
<p>Starting from the code you wrote for our last exercise (where you used dask to load your arcos data on your own computer), let’s write some code to analyze the ARCOS data you’ve written on the cloud.</p>
<p><strong>(6)</strong> Our goal today is going to be to find the pharmaceutical company that has shipped the most opioids (<code class="docutils literal notranslate"><span class="pre">CALC_BASE_WT_IN_GM</span> <span class="pre">*</span> <span class="pre">MME_Conversion_Factor</span></code>) in the US. So write some code to identify this company. A few reminders:</p>
<ul class="simple">
<li><p>Remember to work on your analysis code BELOW the cell where you created your <code class="docutils literal notranslate"><span class="pre">Client</span></code> instance (e.g. where you ran <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">Client(amlcluster)</span></code>). You don’t want to keep re-running that.</p></li>
<li><p>Remember that to read the file, you’ll need to pass your Azure Storage Account secrets to the <code class="docutils literal notranslate"><span class="pre">read_csv</span></code> function.</p></li>
<li><p><strong>The same tricks we’ve been practicing before still apply here: start by only reading in the first couple thousand lines for debugging before you start using dask on the full data!</strong></p></li>
<li><p>Just because we’re on a cluster doesn’t mean we have unlimited resources! Each of the computers we’re using is relativley small, so we still want to do things like only load the columns we need.</p></li>
</ul>
<p>Since we can only use <code class="docutils literal notranslate"><span class="pre">dask</span></code> given how we’re connected to the cluster, we need to ask dask to just give us the first few rows as a dataframe. dask doesn’t support the <code class="docutils literal notranslate"><span class="pre">nrows</span></code> keyword, but you can get the same effect using <code class="docutils literal notranslate"><span class="pre">.head(n=rows_to_get)</span></code>. So rather than running:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dask.dataframe</span> <span class="k">as</span> <span class="nn">dd</span>
<span class="n">rows_to_get</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">dd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="n">rows_to_get</span><span class="p">)</span>
</pre></div>
</div>
<p>You can just run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dask.dataframe</span> <span class="k">as</span> <span class="nn">dd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">dd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">rows_to_get</span><span class="p">)</span>
</pre></div>
</div>
<p>and <code class="docutils literal notranslate"><span class="pre">df</span></code> will be a pandas DataFrame. Note that if you try and use the <code class="docutils literal notranslate"><span class="pre">nrows</span></code> keyword, dask will kindly remind you to use <code class="docutils literal notranslate"><span class="pre">.head()</span></code> instead, so if you forget, don’t worry about it.</p>
<p><strong>(7)</strong> Now let’s run our full dataset on dask, calculating the total shipments by reporting company. Remember:</p>
<ul class="simple">
<li><p>Start by spinning up a cluster</p></li>
<li><p>Dask won’t read compressed files, so you have to unzip your ARCOS data.</p></li>
<li><p>Start your cluster in a cell all by itself since you don’t want to keep re-running the “start a cluster” code.</p></li>
<li><p>Don’t load columns you don’t need!</p></li>
</ul>
<p>As you run your code, make sure to click on the Dashboard link below where you created your cluster:</p>
<p><img alt="dask_dashboard" src="../_images/dask_dashboard.png" /></p>
<p>Among other things, the bar across the bottom should give you a sense of how long your task will take:</p>
<p><img alt="dask_progress" src="../_images/dask_progress.png" /></p>
<p><strong>(8)</strong> Now let’s calculate, <em>for each state</em>, what company shipped the most pills?</p>
<p>Note you will quickly find that you can’t sort in dask – sorting in parallel is <em>really</em> tricky! So you’ll have to work around that. Do what you need to do on the big dataset first, then compute it all so you get it as a regular pandas dataframe, then finish.</p>
<p>Does this seem like a situation where a single company is responsible for the opioid epidemic?</p>
<p><strong>(9)</strong> Now go ahead and try and re-do the chunking you did by hand for your project using dask – calculate, for each year, the total opioids sent to each county in the US (note that I’ve included <code class="docutils literal notranslate"><span class="pre">year</span></code> as its own variable so you don’t have to parse <code class="docutils literal notranslate"><span class="pre">TRANSACTION_DATE</span></code>).</p>
<p><strong>(10)</strong> In several of the preceding analyses, we loaded nearly the same data before doing slightly different data wrangling manipulations. In these situations, it is often best to not run these manipulations separately, but instead to run the common component of the analyses (here, loading the data), then caching that intermediate data using the <code class="docutils literal notranslate"><span class="pre">.persist()</span></code> method (e.g. <code class="docutils literal notranslate"><span class="pre">df</span> <span class="pre">=</span> <span class="pre">client.persist(df)</span></code>).</p>
<p>Take the code you’ve already written to try and generate answers to questions 7, 8, and 9 <em>efficiently</em> using <code class="docutils literal notranslate"><span class="pre">persist</span></code>. If you have time, you can then run each of your answers above again and time them, then compare that total run time to how long your new code (that uses <code class="docutils literal notranslate"><span class="pre">persist</span></code>) takes.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Practical DS</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../class_schedule.html">CLASS SCHEDULE</a></li>
</ul>
<p class="caption"><span class="caption-text">PYTHON &amp; PANDAS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../setup_environment.html">Setting Up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../managing_python_packages.html">Managing Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_v_r.html">Python / R Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vars_v_objects.html">Python: Vars v Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ints_and_floats.html">Numbers in Computers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pandas_series.html">Pandas 1: Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pandas_dataframes.html">Pandas 2: DataFrames</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting_part1.html">Plotting, Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting_part2.html">Plotting, Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../views_and_copies_in_pandas.html">Pandas 3: Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cleaning_editingvalues.html">Cleaning: Editing Values</a></li>
</ul>
<p class="caption"><span class="caption-text">OTHER TOOLS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../command_line_part1.html">Command Line, Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../command_line_part2.html">Command Line, Advanced</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter.html">Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../git_and_github.html">Git and Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pr_review.html">Reviewing Code on Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parquet.html">Parquet Format</a></li>
</ul>
<p class="caption"><span class="caption-text">CLOUD</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cloud_what_is_it.html">What Is The Cloud?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_storage_on_azure.html">Storage on Azure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_vm_on_azure.html">Setup a VM on Azure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_cluster_on_azure.html">Setup a Cluster on Azure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_dask.html">Dask on Azure Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_azure_cli.html">Azure Command Line Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_authentication.html">Azure Authentication</a></li>
</ul>
<p class="caption"><span class="caption-text">SKILLS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_help.html">Getting Help Online</a></li>
<li class="toctree-l1"><a class="reference internal" href="../defensive_programming.html">Defensive Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow.html">Workflow Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../what_is_big_data.html">What is Big Data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big_data_strategies.html">Working with Big Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_understanding.html">Understanding Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_solutions.html">Solving Performance Probs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelism.html">Parallel Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_computing.html">Distributed Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backwards_design.html">Backwards Design</a></li>
</ul>
<p class="caption"><span class="caption-text">OTHER</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../buying_datascience_computer.html">Buying a Data Science Computer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../not_a_mids_student.html">Not a MIDS Student?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheatsheets.html">Cheat Sheets</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Nick Eubank.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/exercises/Exercise_azure_arcos.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
   
    
  </body>
</html>